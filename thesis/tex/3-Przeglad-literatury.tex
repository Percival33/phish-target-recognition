\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Przegląd literatury}
\todo[inline]{więcej napisać o samych pracach}
W tym rozdziale przedstawiony zostanie przegląd literatury naukowej w kontekście istniejących już rozwiązań problemu rozpoznawania stron phishingowych. Zostaną również opisane zbiory danych dołączone do prac.

\subsection{Detekcja stron phishingowych bazująca na haszach i deskryptorach wizualnych}
W pracy \cite{EMD} zastosowano porównywanie wizualne stron, które polega na równywaniu zrzutów ekranu, zmniejszając je i obliczając podobieństwo, korzystając z metryki \textit{Earth Mover’s Distance}.
\todo[inline]{wyjasnić Ad 3.1 Proszę wyjaśnić co to jest EMD}


\subsubsection{Deskryptory wizualne}
Innym podejściem, wartym uwagi, są rozwiązania ekstrahujące deskryptory wizualne \cite{Daisy}. Deskryptory wizualne są to histogramy gradientów otaczających punkt na obrazie. Oprócz tego wykorzystywany jest również algorytm SIFT (ang. \emph{Scale Invariant Feature Transform})\cite{PhishZoo}. Przekształca on zdjęcia w zbiór wektorów cech i na tej podstawie porównuje loga firm w celu rozpoznania podszywanej strony.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\linewidth]{sphx_glr_plot_daisy_001.png}
    \caption{Deskryptory DAISY \cite{scikit-image-daisy}}
    \label{fig:Deskryptory DAISY}
\end{figure}

Oba te rozwiązania później są wykorzystywane do zastosowań \textit{bag of features}, które polegają na klastrowaniu otrzymanych deskryptorów, by utworzyć reprezentację obrazów i porównywać nie same obrazy, a deskryptory obliczone na nich. 
\todo[inline]{scareware i sift}

\subsection{Sieci neuronowe}

Jeśliby traktować rozpoznawanie zrzutów ekranu jako zwykłe zadanie klasyfikacji, to dowolna modyfikacja strony będzie skutkowała uzyskaniem niepoprawnych wyników. Wynika to z faktu, że większość treści w internecie jest dynamiczna, skierowana do konkretnego użytkownika. Przykładami dynamicznej treści są rekomendacje treści czy produktów w sklepach internetowych.

Chcąc rozwiązać ten problem, badacze skupili się na rozwiązaniach bardziej uniwersalnych, charakteryzujących się większą elastycznością. Poniżej zostaną przytoczone dwa podejścia, opisujące rozpoznawanie logotypów oraz grupowanie zrzutów ekranu stron internetowych. Cechą wspólną obu podejść jest posiadanie zbioru podmiotów chronionych. Wykorzystuje się go do określenia, czy próbka podszywa się pod jeden z elementów tego zbioru. Jeśli tak, próbka jest uznana za fałszywą. W przypadku, gdy nie dopasowano podmiotu takowej próbce jest ona uznawana za prawdziwą. W obu przypadkach decyzja o klasyfikacji jest podejmowana na podstawie odległości między badaną próbką a zestawem stron chronionych.

\textbf{Grupowanie zrzutów ekranu}

\todo[inline]{Co? Z tego wynika? Jak?}

W pracy \cite{VisualPhishNet} autorzy skupiają się na nauczeniu modelu, uniwersalnej reprezentacji stron podmiotów. Przykładowo strona główna i strona logowania mogą różnić się wyglądem, lecz dalej jest to ten sam podmiot. Zatem model powinien je klasyfikować tak samo. Niesie to za sobą potrzebę zrzutów ekranu stron chronionych. Aby to osiągnąć, zastosowano proces dotrenowywania, opierając się na modelu bazowym \todo[inline]{jak opisać fine tuning?} VGG16, w połączeniu z siecią syjamską, składającą się z trzech podsieci neuronowych. Jako funkcję straty wykorzystano funkcję \textit{triplet loss}. Trenowanie modelu odbywa się dwustopniowo. Na pierwszym etapie następuje losowe dobieranie przykładów. W drugim etapie sieć jest uczona na "trudnych" przykładach, tzn. tych, które zostały niepoprawnie sklasyfikowane. Podczas klasyfikacji obliczana jest odległość między zanurzeniami analizowanego zrzutu ekranu a zanurzeniem każdego przykładu ze zbioru chronionego. Cel ataku jest rozpoznawany jako podmiot przykładu o najmniejszej odległości od badanej próbki.

\missingfigure{przykład różnych wersji stron tego samego podmiotu} 

Szczegółowa analiza zbioru danych znajduje się niżej w sekcji zbiory danych.
\todo[inline]{opis zbioru i link}
\todo[inline]{opisać co jest na wejściu każdej z metod i na wyjściu}
\textbf{Porównywanie logotypów}
Natomiast, w systemie \textit{Phishpedia} \cite{Phishpedia} autorzy stwierdzają, że nauczenie modelu uniwersalnych profili wielu podmiotów jest zadaniem trudnym, skupiając się wobec tego na innych wyróżnikach. Takowym okazuje się być logo podmiotu. Dzięki temu, model nie wymaga treningu na stronach phishingowych a do stworzenia zbioru stron chronionych wymaga jedynie logotypów i adresu URL dla każdego przykładu zbioru chronionego. System ten składa się z dwóch modeli współpracujących ze sobą. Najpierw model FasterR-CNN\cite{FasterR-CNN} zaznacza wszystkie obszary zainteresowania, w których potencjalnie mogą występować logotypy. Następnie wybiera obszar o największej pewności, by porównać jego zawartość z  chronionymi przykładami. Porównania dokonuje sieć syjamska, wytrenowana wcześniej na zbiorze \cite{Logo2K}, przy wykorzystaniu modelu bazowego Resnetv2 opierając się na kolorowych logotypach. Dzięki skorzystaniu z \textit{transfer learning} \todo[inline]{wytłumaczyć transfer learning i dodać do pojęć} model jest w stanie nauczyć się podobieństwa między logotypami, nie wymuszając nauczenia różnych logo w ten sam sposób. \todo[inline]{powtórzenia - "logo"}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{Rozne logo.png}
    \caption{Przykład wariantów logo jednego podmiotu - firmy Adobe z \cite{Phishpedia}}
    \label{fig:rozne-logo}
\end{figure}

% skupiają się na rozpoznawaniu logo firmy lub wyuczeniu modelu, tak by zbudować uniwersalny profil danej strony poprzez mierzenie odległości między prawdziwą a fałszywą stroną.

% Oba podejścia wymagają sporej elastyczności, a jednym z powodów jest 
% współistnienie wielu wariantów logo opisujących tę samą firmę, jak widać na rysunku poniżej:





% Natomiast \cite{VisualPhishNet} opisuje wykorzystanie sieci syjamskich z trzema podsieciami neuronowymi, które na tej samej zasadzie próbują nauczyć się uniwersalnej reprezentacji stron internetowych. Na pierwszym etapie następuje losowe dobieranie przykładów. W drugim etapie, sieć jest uczona na 'trudnych' przykładach, tzn. tych, które zostały niepoprawnie sklasyfikowane. Podczas klasyfikacji obliczana jest odległość między analizowanym zrzutem ekranu a każdą z monitorowanych stron. Cel ataku jest rozpoznawany jako strona o najmniejszej odległości.

% Wobec tego, autorzy\cite{Phishpedia} wykorzystują sieci syjamskie, które polegają na wytrenowaniu sieci na jednym zadaniu, a następnie wzięciu wytrenowanego modelu i zastosowaniu go w innym zadaniu. Sieć jest trenowana jednocześnie na zbiorze Logo-2K+\cite{Logo2K} oraz logach ze zbioru danych, używając odległości kosinusowej, by nauczyć się podobieństwa logotypów. W ten sposób model nie jest zmuszany do wyznaczenia uniwersalnej reprezentacji różnych logo tej samej firmy. Następnie porównywane są logotypy referencyjne z tymi obecnymi na analizowanej stronie i obliczane jest podobieństwo między nimi. Do tego celu jest wykorzystywany model ResNetv2 \cite{resnetv2}.
% \cite{KnowYourPhish}

% \cite{Phishpedia}
% \cite{PhishZoo}
% \cite{PhishWHO}

% \cite{VisualPhishNet}
% \cite{EMD}

\subsection{Zbiory danych}
% Zadanie rozpoznawania celu ataku phishingu jest bardzo trudne. Uwaga jest skierowana na bardziej znane podmioty, opierając się na intuicji, iż atakujący dążą do maksymalizowania zysków z ataków, wobec czego skupiają się na podszywaniu się pod instytucje finansowe i znane marki\cite{PhishingArticle}
% % (TODO: Tyler Moore. Phishing and the economics of e-crime.Infosecurity, 4(6):34–37, 2007) 
% takie jak: firmy kurierskie, urzędy administracji, operatorów telekomunikacyjnych, czy nawet znajomych użytkownika, starają się wyłudzić dane do logowania np. do kont bankowych lub używanych przez atakowanego kont społecznościowych, czy systemów biznesowych.\cite{govPhishing}

% Obecnie strony internetowe są często aktualizowane. Zmieniane są czcionki, układ strony i inne elementy wizualne. Ponadto zawierają dynamiczne treści, co utrudnia identyfikację. Kolejnym elementem jest fakt, że firmy mogą mieć logo w różnych wariantach oraz różne domeny np. "amazon.pl" oraz "amazon.co.jp" w zależności od kraju. To utrudnia pracę, zwiększając liczbę wariantów stron należących do tego samego podmiotu, wymuszając większą liczbę stron, które należy analizować.

% Jak zauważają autorzy pracy \cite{Phishpedia} liczba stron wcale nie musi być taka duża. Gdyż w zbiorze około 30 000 stron phishingowych uzyskanych z systemu OpenPhish\footnote{\href{https://www.openphish.com/}{https://www.openphish.com/}}, 100 najpopularniejszych marek pokrywa aż 95.8\% przykładów. Potwierdzając intuicję o tym, że atakujący wybierają jako cel ataku popularniejsze strony.

Zadanie rozpoznawania celu ataku phishingowego jest skomplikowane między innymi ze względu na dynamikę zmian. Przykładami takich zmian są zmiany czcionek czy zmiana kolorów marki. Dodatkowo występuje problem różnic w układach stron zależnie od języka a także spójne rozpoznawanie celu podszycia w zależności od domen. Są to przykładowo "amazon.pl" oraz "amazon.co.jp", które są stronami tego samego podmiotu, tutaj firmy Amazon. 

Zbiory danych udostępnione kilka lat temu szybko się dezaktualizują, z powodu innych wyglądów stron internetowych. Stworzenie zbioru danych jest też trudne ze względu na ogromną liczbę różnych podmiotów i instytucji, które mogą być atakowane. Autorzy pracy \cite{PhishingArticle} zaznaczają, że celami ataku są głównie podmioty, których atakowanie przyniesie jak największy zysk. Zatem są to najczęściej instytucje finansowe, urzędy administacji, sklepy internetowe, firmy kurierskie czy platformy społecznościowe.\cite{govPhishing}

Jak zauważają autorzy pracy Phishpedia \cite{Phishpedia} liczba podmiotów wartych uwagi atakujących nie musi być taka duża. Jest tak ze względu na to, że w zbiorze około 30 000 stron phishingowych uzyskanych z systemu OpenPhish\footnote{\href{https://www.openphish.com/}{https://www.openphish.com/}}, wybierając 100 najpopularniejszych marek pokrywa aż 95.8\% przykładów. Potwierdzając tym samym intuicję, że atakujący kierują się popularnością przy wybieraniu celu ataku.

\textbf{Porównanie zbiorów danych z prac \textit{VisualPhishNet} i \textit{Phishpedia}}
Zbiory danych w obu artykułach naukowych zawierają zrzuty ekranu różnych stron internetowych. Są to zgodnie z poprzednimi obserwacjami strony banków, technologiczne (microsorft, apple, dropbox) jak strony podmiotów telekomunikacyjnych i sklepów internetowych. Autorzy podzielili dane ze względu na okoliczność ich uzyskania. Są to: "benign\_test", "browsers", "newly\_crawled\_phishing", "pages\_with\_trusted\_logos", "phishing" i "trusted\_list". W procesie uczenia wykorzystywano zdjęcia z folderów "phishing" i "trusted\_list" natomiast do ewaluacji modeli wykorzystano "newly\_crawled\_phishing".

\missingfigure{plot bar top 20 podmiotów z każdego zbioru danych}

\missingfigure{tableka / wykres liczności zbiorów danych z uwzględnieniem benign / phish}


W przypadku zbioru phishpedia, są to dwa zbiory danych stron bezpiecznych i phishingowych mających odpowiednio 29k i 30k przykładów. Każdy z nich jest opisany adresem URL i zrzut ekranu a wybrane posiadają również kod źródłowy html, ocr strony,  oraz koordynaty logotypu. Warto zwrócić uwagę że zbiór stron bezpiecznych zawiera 30649 różnych stron a zbiór phishingowy 29496 przykładów, które opisują 283 różne podmioty.
\todo[inline]{zebrać ile jest unikalnych celi podszycia w zbiorze phishingowym}

\missingfigure{plot bar top 20 podmiotów z każdego zbioru danych}

\missingfigure{tableka / wykres liczności zbiorów danych z uwzględnieniem benign / phish}





\todo[inline]{zbior stron chronionych najpopularniejsze marki w obu zbiorach danych}

Dopasowywanie podmiotów występujących w obu zbiorach danych zostało wykonane w następujących krokach:
\todo[inline]{opis tej metody użytej do analizy i do przypisania celi podszycia w końcowej analizie - LLM? i analiza wyników / miary} 


\todo[inline]{ile podmiotów występuje w obu zbiorach danych i który to jest notebook} 



vp - chronione 9k, phish (train/test) 1k