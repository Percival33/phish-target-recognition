{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f103fb0776c84526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:08:01.822849Z",
     "start_time": "2025-01-11T12:08:01.818145Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:29:23.804738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 00:29:24.490464: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-20 00:29:24.490539: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-20 00:29:24.490547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,Subtract,Reshape\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from skimage.io import imsave\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tools.config import PROCESSED_DATA_DIR, INTERIM_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca455707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.11.0', '2.11.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd68d1cfd4ece7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T11:43:51.213241Z",
     "start_time": "2025-01-11T11:43:51.205253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_38933/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:20:30.609450: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 00:20:30.684350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:30.712491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:30.712540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:31.413638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:31.413724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:31.413732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-01-20 00:20:31.413767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:20:31.413795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca7f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:29:28.451084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:29:28.469687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:29:28.469743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587e65bf150e5182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:22:28.121966Z",
     "start_time": "2025-01-11T12:22:28.114515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "\"\"\"\n",
    "dataset_path = 'smallerSampleDataset/'\n",
    "reshape_size = [224,224,3]\n",
    "phishing_test_size = 0.4\n",
    "num_targets = 155\n",
    "\"\"\"\n",
    "\n",
    "dataset_config = {\n",
    "    \"dataset_path\": PROCESSED_DATA_DIR / 'smallerSampleDataset/',\n",
    "    \"reshape_size\": [224, 224, 3],\n",
    "    \"phishing_test_size\": 0.4,\n",
    "    \"num_targets\": 5\n",
    "}\n",
    "\n",
    "# Model parameters\n",
    "\"\"\"\n",
    "input_shape = [224,224,3]\n",
    "margin = 2.2\n",
    "new_conv_params = [5,5,512]\n",
    "\"\"\"\n",
    "\n",
    "model_config = {\n",
    "    'input_shape': [224, 224, 3],\n",
    "    'margin': 2.2,\n",
    "    'new_conv_params': [5, 5, 512]\n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "\"\"\" original params\n",
    "start_lr = 0.00002\n",
    "output_dir = '../../../notebooks/'\n",
    "saved_model_name = 'model'\n",
    "save_interval = 2000\n",
    "batch_size = 32\n",
    "n_iter = 21000\n",
    "lr_interval = 100\n",
    "\"\"\"\n",
    "\n",
    "training_config = {\n",
    "    \"start_lr\": 0.00002,\n",
    "    'output_dir': INTERIM_DATA_DIR / 'smallerSampleDataset',\n",
    "    \"saved_model_name\": 'model',\n",
    "    'save_interval': 200,\n",
    "    \"batch_size\": 16,  \n",
    "    \"n_iter\": 210,\n",
    "    \"lr_interval\":100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "dataset_path = dataset_config['dataset_path']\n",
    "reshape_size = dataset_config['reshape_size']\n",
    "phishing_test_size = dataset_config['phishing_test_size']\n",
    "num_targets = dataset_config['num_targets']\n",
    "\n",
    "# Model parameters\n",
    "input_shape = model_config['input_shape']\n",
    "margin = model_config['margin']\n",
    "new_conv_params = model_config['new_conv_params']\n",
    "\n",
    "# Training parameters\n",
    "start_lr = training_config['start_lr']\n",
    "output_dir = training_config['output_dir']\n",
    "saved_model_name = training_config['saved_model_name']\n",
    "save_interval = training_config['save_interval']\n",
    "batch_size =training_config['batch_size']\n",
    "n_iter = training_config['n_iter']\n",
    "lr_interval = training_config['lr_interval']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6792c6cc68cbbfa",
   "metadata": {},
   "source": [
    "# Load dataset:\n",
    "    - Load training screenshots per website\n",
    "    - Load Phishing screenshots per website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f198ccb8110d6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:14:39.967202Z",
     "start_time": "2025-01-11T12:14:39.960937Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "\n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path + targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path+'/'+file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions\n",
    "                try:\n",
    "                    img = imread(target_path+'/'+file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break\n",
    "    return all_imgs,all_labels,all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84276bbda72fc59d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T12:22:31.148231Z",
     "start_time": "2025-01-11T12:22:31.095287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'trusted_list/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "# imgs_num = 9363\n",
    "imgs_num = 420\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "# imgs_num = 1195\n",
    "imgs_num = 160\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Split phishing to training and test, load train and test indices.\n",
    "idx_test = np.load(output_dir+'test_idx.npy')\n",
    "idx_train = np.load(output_dir+'train_idx.npy')\n",
    "X_test_phish = all_imgs_test[idx_test,:]\n",
    "y_test_phish = all_labels_test[idx_test,:]\n",
    "X_train_phish = all_imgs_test[idx_train,:]\n",
    "y_train_phish = all_labels_test[idx_train,:]\n",
    "\n",
    "#otherwise, make a new split here.\n",
    "\n",
    "# idx = np.arange(all_imgs_test.shape[0])\n",
    "# X_test_phish, X_train_phish, y_test_phish, y_train_phish,idx_test,idx_train = train_test_split(all_imgs_test, all_labels_test,idx, test_size=phishing_test_size)\n",
    "# np.save(output_dir+'test_idx',idx_test)\n",
    "# np.save(output_dir+'train_idx',idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(all_imgs_train, output_dir+'all_imgs_train')\n",
    "joblib.dump(all_labels_train, output_dir+'all_labels_train')\n",
    "\n",
    "joblib.dump(all_imgs_test, output_dir+'all_imgs_test')\n",
    "joblib.dump(all_labels_test, output_dir+'all_labels_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc2a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# # Split phishing to training and test, load train and test indices.\n",
    "# all_imgs_train = joblib.load(output_dir/'all_imgs_train')\n",
    "# all_labels_train = joblib.load(output_dir/+'all_labels_train')\n",
    "\n",
    "# all_imgs_test = joblib.load(output_dir/'all_imgs_test')\n",
    "# all_labels_test = joblib.load(output_dir/'all_labels_test')\n",
    "\n",
    "idx_test = np.load(output_dir/'test_idx.npy')\n",
    "idx_train = np.load(output_dir/'train_idx.npy')\n",
    "\n",
    "imgs_train_path = output_dir / 'all_imgs_train.npy'\n",
    "labels_train_path = output_dir / 'all_labels_train.npy'\n",
    "file_names_train_path = output_dir / 'all_file_names_train.npy'\n",
    "\n",
    "imgs_test_path = output_dir / 'all_imgs_test.npy'\n",
    "labels_test_path = output_dir / 'all_labels_test.npy'\n",
    "file_names_test_path = output_dir / 'all_file_names_test.npy'\n",
    "\n",
    "all_imgs_train = np.load(imgs_train_path)\n",
    "all_labels_train = np.load(labels_train_path)\n",
    "all_file_names_train = np.load(file_names_train_path)\n",
    "\n",
    "all_imgs_test = np.load(imgs_test_path)\n",
    "all_labels_test = np.load(labels_test_path)\n",
    "all_file_names_test = np.load(file_names_test_path)\n",
    "\n",
    "X_test_phish = all_imgs_test[idx_test,:]\n",
    "y_test_phish = all_labels_test[idx_test,:]\n",
    "X_train_phish = all_imgs_test[idx_train,:]\n",
    "y_train_phish = all_labels_test[idx_train,:]\n",
    "\n",
    "data_path = dataset_path / 'phishing'\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea5879f14d2f98",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8774a59a9c4fda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_triplet_network(input_shape, new_conv_params):\n",
    "\n",
    "    # Input_shape: shape of input images\n",
    "    # new_conv_params: dimension of the new convolution layer [spatial1,spatial2,channels]\n",
    "\n",
    "    # Define the tensors for the three input images\n",
    "    anchor_input = Input(input_shape)\n",
    "    positive_input = Input(input_shape)\n",
    "    negative_input = Input(input_shape)\n",
    "\n",
    "    # Use VGG as a base model\n",
    "    base_model = VGG16(weights='imagenet',  input_shape=input_shape, include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Conv2D(new_conv_params[2],(new_conv_params[0],new_conv_params[1]),activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(2e-4)) (x)\n",
    "    x = GlobalMaxPooling2D() (x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_a = model(anchor_input)\n",
    "    encoded_p = model(positive_input)\n",
    "    encoded_n = model(negative_input)\n",
    "\n",
    "    mean_layer = Lambda(lambda x: K.mean(x,axis=1))\n",
    "\n",
    "    square_diff_layer = Lambda(lambda tensors:K.square(tensors[0] - tensors[1]))\n",
    "    square_diff_pos = square_diff_layer([encoded_a,encoded_p])\n",
    "    square_diff_neg = square_diff_layer([encoded_a,encoded_n])\n",
    "\n",
    "    square_diff_pos_l2 = mean_layer(square_diff_pos)\n",
    "    square_diff_neg_l2 = mean_layer(square_diff_neg)\n",
    "\n",
    "    # Add a diff layer\n",
    "    diff = Subtract()([square_diff_pos_l2, square_diff_neg_l2])\n",
    "    diff = Reshape((1,)) (diff)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    triplet_net = Model(inputs=[anchor_input,positive_input,negative_input],outputs=diff)\n",
    "\n",
    "    # return the model\n",
    "    return triplet_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4040fc7de1becec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:30:00.165263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-20 00:30:00.166786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:00.166891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:00.166923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:01.258527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:01.258600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:01.258609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-01-20 00:30:01.258642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-20 00:30:01.258674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3906 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 512)          21268800    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 512)          0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]',                  \n",
      "                                                                  'model[0][0]',                  \n",
      "                                                                  'model[2][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None,)              0           ['lambda_1[0][0]',               \n",
      "                                                                  'lambda_1[1][0]']               \n",
      "                                                                                                  \n",
      " subtract (Subtract)            (None,)              0           ['lambda[0][0]',                 \n",
      "                                                                  'lambda[1][0]']                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['subtract[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,268,800\n",
      "Trainable params: 21,268,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarcin/inz/src/models/visualphishnet/.venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the triplet loss\n",
    "# Create and compile model\n",
    "\n",
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "model = define_triplet_network(input_shape, new_conv_params)\n",
    "model.summary()\n",
    "\n",
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "model.compile(loss=custom_loss(margin),optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe4c1e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_1, Trainable: True\n",
      "Layer: input_2, Trainable: True\n",
      "Layer: input_3, Trainable: True\n",
      "Layer: model, Trainable: True\n",
      "Layer: lambda_1, Trainable: True\n",
      "Layer: lambda, Trainable: True\n",
      "Layer: subtract, Trainable: True\n",
      "Layer: reshape, Trainable: True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(f\"Layer: {layer.name}, Trainable: {layer.trainable}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec271d40ede24508",
   "metadata": {},
   "source": [
    "# Triplet Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c880c705e7ecefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order random phishing arrays per website (from 0 to 155 target)\n",
    "\n",
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,:,:] = orig_arr[j,:,:,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr\n",
    "\n",
    "X_test_phish,y_test_phish = order_random_array(X_test_phish,y_test_phish,num_targets)\n",
    "X_train_phish,y_train_phish = order_random_array(X_train_phish,y_train_phish,num_targets)\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the phishing set (used later in triplet sampling)\n",
    "# Not all targets might be in the phishing set\n",
    "def targets_start_end(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "\n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_phish = targets_start_end(num_targets,y_train_phish)\n",
    "labels_start_end_test_phish = targets_start_end(num_targets,y_test_phish)\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the training set (used later in triplet sampling)\n",
    "def all_targets_start_end(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = all_targets_start_end(num_targets,y_train_legit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ec2431d607680ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample anchor, positive and negative images\n",
    "def pick_first_img_idx(labels_start_end,num_targets):\n",
    "    random_target = -1\n",
    "    while (random_target == -1):\n",
    "        random_target = np.random.randint(low = 0,high = num_targets)\n",
    "        if labels_start_end[random_target,0] == -1:\n",
    "            random_target = -1\n",
    "    return random_target\n",
    "\n",
    "def pick_pos_img_idx(prob_phish,img_label):\n",
    "    if np.random.uniform() > prob_phish:\n",
    "        class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "        same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "        img = X_train_legit[same_idx,:]\n",
    "    else:\n",
    "        if not labels_start_end_train_phish[img_label,0] == -1:\n",
    "            class_idx_start_end = labels_start_end_train_phish[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_phish[same_idx,:]\n",
    "        else:\n",
    "            class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_legit[same_idx,:]\n",
    "    return img\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "\n",
    "    class_idx_start_end = labels_start_end_train_legit[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_legit[idx_from_diff_target,:]\n",
    "\n",
    "    return img,diff_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b95a127651c60",
   "metadata": {},
   "source": [
    "# Sample batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295d0cd33c236d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't sample negative image from the same category as the positive image (e.g. google and google drive)\n",
    "# Create clusters of same-company websites (e.g. all microsoft websites)\n",
    "\n",
    "\n",
    "targets_file = open(data_path/'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "\n",
    "#targets names of parent and sub websites\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx\n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc01afe6aefc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample triplets\n",
    "def get_batch(batch_size,num_targets):\n",
    "\n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train_legit,num_targets)\n",
    "        triple[0][i,:,:,:] = X_train_legit[img_idx_pair1,:]\n",
    "        img_label = int(y_train_legit[img_idx_pair1])\n",
    "\n",
    "        # get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(0.15,img_label)\n",
    "\n",
    "        # get image for the thrid: negative from legit\n",
    "        # don't sample from the same cluster\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "\n",
    "    return triple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd920340523c19b0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc835de940673a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model):\n",
    "    model.save(output_dir/f\"{saved_model_name}.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a5baff2f317429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 00:30:15.838850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904\n",
      "2025-01-20 00:30:17.210758: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.210809: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.210818: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.210824: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.313894: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.313947: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.313957: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.313963: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.455774: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:17.455826: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-01-20 00:30:19.123785: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 1. Loss: 2.402111291885376\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 2. Loss: 2.394646406173706\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 3. Loss: 2.375261068344116\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 4. Loss: 2.372143030166626\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 5. Loss: 2.3833775520324707\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 6. Loss: 2.346803903579712\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 7. Loss: 2.317201614379883\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 8. Loss: 2.3280229568481445\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 9. Loss: 2.1926705837249756\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 10. Loss: 2.174473762512207\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 11. Loss: 2.1394362449645996\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 12. Loss: 2.011240005493164\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 13. Loss: 1.6103100776672363\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 14. Loss: 1.9141367673873901\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 15. Loss: 0.9742411375045776\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 16. Loss: 1.5117285251617432\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 17. Loss: 0.5781219005584717\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 18. Loss: 0.5359830856323242\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 19. Loss: 0.45177263021469116\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 20. Loss: 0.6474122405052185\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 21. Loss: 0.3959752023220062\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 22. Loss: 0.20226415991783142\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 23. Loss: 0.4255565404891968\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 24. Loss: 0.33623719215393066\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 25. Loss: 0.3577325940132141\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 26. Loss: 1.0062988996505737\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 27. Loss: 0.25873520970344543\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 28. Loss: 0.22751662135124207\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 29. Loss: 0.21900364756584167\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 30. Loss: 0.2807331085205078\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 31. Loss: 0.38191694021224976\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 32. Loss: 0.40540194511413574\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 33. Loss: 0.2802133560180664\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 34. Loss: 0.20142188668251038\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 35. Loss: 0.4898168444633484\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 36. Loss: 0.20128381252288818\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 37. Loss: 0.40760406851768494\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 38. Loss: 0.5235182046890259\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 39. Loss: 0.2010778784751892\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 40. Loss: 0.20100946724414825\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 41. Loss: 0.3434293568134308\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 42. Loss: 0.2008722424507141\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 43. Loss: 0.3414410352706909\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 44. Loss: 0.2007347047328949\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 45. Loss: 0.20066574215888977\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 46. Loss: 0.226565882563591\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 47. Loss: 0.3162078857421875\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 48. Loss: 0.200459286570549\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 49. Loss: 0.20039065182209015\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 50. Loss: 0.20032183825969696\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 51. Loss: 0.6569314002990723\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 52. Loss: 0.44358915090560913\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 53. Loss: 0.20011845231056213\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 54. Loss: 0.20005163550376892\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 55. Loss: 0.19998472929000854\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 56. Loss: 0.20026499032974243\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 57. Loss: 0.19985134899616241\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 58. Loss: 0.20745599269866943\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 59. Loss: 0.3859744071960449\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 60. Loss: 0.19965308904647827\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 61. Loss: 0.29871198534965515\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 62. Loss: 0.19952209293842316\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 63. Loss: 0.19945675134658813\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 64. Loss: 0.19939139485359192\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 65. Loss: 0.19932590425014496\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 66. Loss: 0.19926036894321442\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 67. Loss: 0.19919472932815552\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 68. Loss: 0.19912903010845184\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 69. Loss: 0.3504592478275299\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 70. Loss: 0.25024348497390747\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 71. Loss: 0.1989327073097229\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 72. Loss: 0.32370299100875854\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 73. Loss: 0.19880308210849762\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 74. Loss: 0.1987384855747223\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 75. Loss: 0.20927587151527405\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 76. Loss: 0.19860965013504028\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 77. Loss: 0.19854538142681122\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 78. Loss: 0.19848106801509857\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 79. Loss: 0.19841672480106354\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 80. Loss: 0.19835230708122253\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 81. Loss: 0.19828787446022034\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 82. Loss: 0.19822339713573456\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 83. Loss: 0.1981588751077652\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 84. Loss: 0.19809430837631226\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 85. Loss: 0.19802972674369812\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 86. Loss: 0.1979651004076004\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 87. Loss: 0.1979004144668579\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 88. Loss: 0.19783572852611542\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 89. Loss: 0.19777102768421173\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 90. Loss: 0.19770629703998566\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 91. Loss: 0.197641521692276\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 92. Loss: 0.19757674634456635\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 93. Loss: 0.1975119709968567\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 94. Loss: 0.19744718074798584\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 95. Loss: 0.1973823606967926\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 96. Loss: 0.19731755554676056\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 97. Loss: 0.19725272059440613\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 98. Loss: 0.1971879005432129\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 99. Loss: 0.31023991107940674\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 100. Loss: 0.34283560514450073\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 101. Loss: 0.19699473679065704\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 102. Loss: 0.19693143665790558\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 103. Loss: 0.6426532864570618\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 104. Loss: 0.19680523872375488\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 105. Loss: 0.19674235582351685\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 106. Loss: 0.1966795176267624\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 107. Loss: 0.19661669433116913\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 108. Loss: 0.19655387103557587\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 109. Loss: 0.196491077542305\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 110. Loss: 0.19642826914787292\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 111. Loss: 0.19636547565460205\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 112. Loss: 0.32035791873931885\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 113. Loss: 0.1962401270866394\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 114. Loss: 0.3616504371166229\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 115. Loss: 0.1961156725883484\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 116. Loss: 0.2965865135192871\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 117. Loss: 0.1959926187992096\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 118. Loss: 0.19593150913715363\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 119. Loss: 0.307437002658844\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 120. Loss: 0.19580978155136108\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 121. Loss: 0.1957492083311081\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 122. Loss: 0.1956886202096939\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 123. Loss: 0.19562804698944092\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 124. Loss: 0.19556745886802673\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 125. Loss: 0.19550690054893494\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 126. Loss: 0.2647686004638672\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 127. Loss: 0.1953866332769394\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 128. Loss: 0.19532695412635803\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 129. Loss: 0.19526727497577667\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 130. Loss: 0.1952076256275177\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 131. Loss: 0.19514799118041992\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 132. Loss: 0.19508832693099976\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 133. Loss: 0.19502869248390198\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 134. Loss: 0.194969043135643\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 135. Loss: 0.19490940868854523\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 136. Loss: 0.19484977424144745\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 137. Loss: 0.19479016959667206\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 138. Loss: 0.19473056495189667\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 139. Loss: 0.19467096030712128\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 140. Loss: 0.19461137056350708\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 141. Loss: 0.19455179572105408\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 142. Loss: 0.19449222087860107\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 143. Loss: 0.19443269073963165\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 144. Loss: 0.19437316060066223\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 145. Loss: 0.1943136602640152\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 146. Loss: 0.19425417482852936\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 147. Loss: 0.1941947042942047\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 148. Loss: 0.19413527846336365\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 149. Loss: 0.19407586753368378\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 150. Loss: 0.3351929187774658\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 151. Loss: 0.19395795464515686\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 152. Loss: 0.1938994973897934\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 153. Loss: 0.19384106993675232\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 154. Loss: 0.19378265738487244\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 155. Loss: 0.19372428953647614\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 156. Loss: 0.19366592168807983\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 157. Loss: 0.19360759854316711\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 158. Loss: 0.1935492902994156\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 159. Loss: 0.19349101185798645\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 160. Loss: 0.1934327781200409\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 161. Loss: 0.19337455928325653\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 162. Loss: 0.19331635534763336\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 163. Loss: 0.19325819611549377\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 164. Loss: 0.1932000368833542\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 165. Loss: 0.19314193725585938\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 166. Loss: 0.19308385252952576\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 167. Loss: 0.19756440818309784\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 168. Loss: 0.19296859204769135\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 169. Loss: 0.19291144609451294\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 170. Loss: 0.19285434484481812\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 171. Loss: 0.19279727339744568\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 172. Loss: 0.19274024665355682\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 173. Loss: 0.19268321990966797\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 174. Loss: 0.19262626767158508\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 175. Loss: 0.1925693303346634\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 176. Loss: 0.1925124078989029\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 177. Loss: 0.19245553016662598\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 178. Loss: 0.19239866733551025\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 179. Loss: 0.19234183430671692\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 180. Loss: 0.19228504598140717\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 181. Loss: 0.431571364402771\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 182. Loss: 0.2166449874639511\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 183. Loss: 0.1921158879995346\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 184. Loss: 0.1920599490404129\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 185. Loss: 0.19200405478477478\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 186. Loss: 0.19194820523262024\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 187. Loss: 0.19189240038394928\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 188. Loss: 0.19183661043643951\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 189. Loss: 0.19178088009357452\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 190. Loss: 0.19172519445419312\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 191. Loss: 0.1916695088148117\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 192. Loss: 0.19161386787891388\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 193. Loss: 0.19155827164649963\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 194. Loss: 0.1915026605129242\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 195. Loss: 0.19144710898399353\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 196. Loss: 0.19139157235622406\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 197. Loss: 0.19133608043193817\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 198. Loss: 0.19128063321113586\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 199. Loss: 0.19122518599033356\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 200. Loss: 0.19116976857185364\n",
      "Saved model to disk\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 201. Loss: 0.1911143958568573\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 202. Loss: 0.19105961918830872\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 203. Loss: 0.19100487232208252\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 204. Loss: 0.1909501701593399\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 205. Loss: 0.19089548289775848\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 206. Loss: 0.19084082543849945\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 207. Loss: 0.190786212682724\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 208. Loss: 0.19073164463043213\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 209. Loss: 0.19067710638046265\n"
     ]
    }
   ],
   "source": [
    "# run = wandb.init(\n",
    "#     project=\"Inz\",\n",
    "#     notes=f\"VisualPhish smallerSampleDataset\",\n",
    "#     config={**model_config, **dataset_config, **training_config}\n",
    "# )\n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "for i in range(1, n_iter):\n",
    "    inputs=get_batch(batch_size,num_targets)\n",
    "    loss_value=model.train_on_batch(inputs,targets_train)\n",
    "    \n",
    "    print(\"\\n ------------- \\n\")\n",
    "    print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_value))\n",
    "    # run.log({\"loss\": loss_value})\n",
    "\n",
    "    if i % save_interval == 0:\n",
    "        save_keras_model(model)\n",
    "\n",
    "    if i % lr_interval == 0:\n",
    "        start_lr = 0.99*start_lr\n",
    "        K.set_value(model.optimizer.lr, start_lr)\n",
    "        # run.log({\"lr\": start_lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f80a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(output_dir)\n",
    "run.log_model(os.path.join(path, 'model.h5'), name=\"VP-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496817c388a8d68",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca43e5ed81c653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 10s 644ms/step\n",
      "3/3 [==============================] - 3s 1s/step\n"
     ]
    }
   ],
   "source": [
    "shared_model = model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir/'whitelist_emb',whitelist_emb)\n",
    "np.save(output_dir/'whitelist_labels',y_train_legit)\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir/'phishing_emb',phishing_emb)\n",
    "np.save(output_dir/'phishing_labels',all_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf96827",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact=wandb.Artifact(name='VisualPhish-phase-1', type='embeddings')\n",
    "\n",
    "artifact.add_file(os.path.join(output_dir, 'whitelist_emb.npy'), 'whitelist_emb')\n",
    "artifact.add_file(os.path.join(output_dir, 'whitelist_labels.npy'), 'whitelist_labels')\n",
    "artifact.add_file(os.path.join(output_dir, 'phishing_emb.npy'), 'phishing_emb')\n",
    "artifact.add_file(os.path.join(output_dir, 'phishing_labels.npy'), 'phishing_labels')\n",
    "run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6904e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
