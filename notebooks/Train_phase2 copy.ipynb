{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:22:12.102468Z",
     "start_time": "2025-01-20T23:22:12.096762Z"
    }
   },
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:22:13.720199Z",
     "start_time": "2025-01-20T23:22:13.712864Z"
    }
   },
   "source": [
    "from tools.config import INTERIM_DATA_DIR, PROCESSED_DATA_DIR\n",
    "# Dataset parameters \n",
    "dataset_path = PROCESSED_DATA_DIR / 'smallerSampleDataset'\n",
    "reshape_size = [224,224,3]\n",
    "phishing_test_size = 0.4\n",
    "num_targets = 5 \n",
    "\n",
    "# Model parameters\n",
    "input_shape = [224,224,3]\n",
    "margin = 2.2\n",
    "new_conv_params = [5,5,512]\n",
    "\n",
    "# Training parameters\n",
    "start_lr = 0.00002\n",
    "output_dir = INTERIM_DATA_DIR / 'smallerSampleDataset'\n",
    "saved_model_name = 'model' #from first training \n",
    "new_saved_model_name = 'model2'\n",
    "save_interval = 200\n",
    "batch_size = 16 # changed value\n",
    "n_iter = 500\n",
    "lr_interval = 5\n",
    "# hard examples training \n",
    "num_sets = 10\n",
    "iter_per_set = 8\n",
    "n_iter = 30\n",
    "batch_size = 16 # changed value"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:22:21.344342Z",
     "start_time": "2025-01-20T23:22:20.524549Z"
    }
   },
   "source": [
    "import wandb\n",
    "params_dict = {\n",
    "    'dataset_path': str(dataset_path),\n",
    "    'reshape_size': reshape_size,\n",
    "    'phishing_test_size': phishing_test_size,\n",
    "    'num_targets': num_targets,\n",
    "    \n",
    "    'model_params': {\n",
    "        'input_shape': input_shape,\n",
    "        'margin': margin,\n",
    "        'new_conv_params': new_conv_params\n",
    "    },\n",
    "    \n",
    "    'training_params': {\n",
    "        'start_lr': start_lr,\n",
    "        'output_dir': str(output_dir),\n",
    "        'saved_model_name': saved_model_name,\n",
    "        'new_saved_model_name': new_saved_model_name,\n",
    "        'save_interval': save_interval,\n",
    "        'batch_size': batch_size,\n",
    "        'n_iter': n_iter,\n",
    "        'lr_interval': lr_interval\n",
    "    },\n",
    "    \n",
    "    'hard_examples_training': {\n",
    "        'num_sets': num_sets,\n",
    "        'iter_per_set': iter_per_set,\n",
    "        'n_iter': n_iter,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "}\n",
    "# run = wandb.init(\n",
    "#     project=\"Inz\",\n",
    "#     notes=f\"VisualPhish smallerSampleDataset\",\n",
    "#     config={**model_config, **dataset_config, **training_config}\n",
    "# )\n",
    "    # run.log({\"loss\": loss_value})\n",
    "    # run.log({\"lr\": start_lr})\n",
    "# run.log_model(os.path.join(path, 'model.h5'), name=\"VP-Model\")\n",
    "# run.finish()\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset:\n",
    "- Load training screenshots per website\n",
    "- Load Phishing screenshots per website"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:22:24.936689Z",
     "start_time": "2025-01-20T23:22:24.928382Z"
    }
   },
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "    \n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path / targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path / file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions \n",
    "                try:\n",
    "                    img = imread(target_path / file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break \n",
    "    return all_imgs,all_labels,all_file_names\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:26:51.665894Z",
     "start_time": "2025-01-20T23:22:28.310813Z"
    }
   },
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path / 'trusted_list'\n",
    "targets_file = open(data_path/'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 420\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path / 'phishing/'\n",
    "targets_file = open(data_path/'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 160\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Load the same train/split in phase 1\n",
    "phish_test_idx = np.load(output_dir/'test_idx.npy')\n",
    "phish_train_idx = np.load(output_dir/'train_idx.npy')\n",
    "\n",
    "X_test_phish = all_imgs_test[phish_test_idx,:]\n",
    "y_test_phish = all_labels_test[phish_test_idx,:]\n",
    "\n",
    "X_train_phish = all_imgs_test[phish_train_idx,:]\n",
    "y_train_phish = all_labels_test[phish_train_idx,:]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jarcin/inz/data/processed/smallerSampleDataset/trusted_list/absa\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/trusted_list/alibaba\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/trusted_list/amazon\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/trusted_list/apple\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/trusted_list/boa\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/phishing/absa\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/phishing/alibaba\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/phishing/amazon\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/phishing/apple\n",
      "/home/jarcin/inz/data/processed/smallerSampleDataset/phishing/boa\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "all_imgs_train = joblib.load(output_dir+'all_imgs_train')\n",
    "all_labels_train = joblib.load(output_dir+'all_labels_train')\n",
    "\n",
    "all_imgs_test = joblib.load(output_dir+'all_imgs_test')\n",
    "all_labels_test = joblib.load(output_dir+'all_labels_test')\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "idx_test = np.load(output_dir+'test_idx.npy')\n",
    "idx_train = np.load(output_dir+'train_idx.npy')\n",
    "X_test_phish = all_imgs_test[idx_test,:]\n",
    "y_test_phish = all_labels_test[idx_test,:]\n",
    "X_train_phish = all_imgs_test[idx_train,:]\n",
    "y_train_phish = all_labels_test[idx_train,:]\n",
    "\n",
    "data_path = dataset_path + 'phishing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order and label targets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:26:51.723528Z",
     "start_time": "2025-01-20T23:26:51.713543Z"
    }
   },
   "source": [
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,:,:] = orig_arr[j,:,:,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr \n",
    "\n",
    "# Store the start and end of each target in the phishing set (used later in triplet sampling)\n",
    "# Not all targets might be in the phishing set \n",
    "def start_end_each_target(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the training set (used later in triplet sampling)\n",
    "def all_targets_start_end(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            #count_target = count_target + 1\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "    \n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            print(i)\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = all_targets_start_end(num_targets,y_train_legit)\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Hard subsets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:26:51.784803Z",
     "start_time": "2025-01-20T23:26:51.766925Z"
    }
   },
   "source": [
    "# Find a query set for each target\n",
    "def find_fixed_set_idx(num_target):\n",
    "    website_random_idx = np.zeros([num_target,])\n",
    "    for i in range(0,num_target):\n",
    "        class_idx_start_end = labels_start_end_train_legit[i,:]\n",
    "        website_random_idx[i] = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    return website_random_idx\n",
    "\n",
    "# Compute L2 distance between embeddings\n",
    "def compute_distance_pair(layer1,layer2):\n",
    "    diff = layer1 - layer2\n",
    "    l2_diff = np.mean(diff**2)\n",
    "    return l2_diff\n",
    "    \n",
    "# Compute the embeddings of the query set, the phishing training set, the training whitelist \n",
    "def predict_all_imgs(model):\n",
    "    X_train_legit_last_layer = model.predict(X_train_legit,batch_size=10)\n",
    "    X_train_phish_last_layer = model.predict(X_train_phish,batch_size=10)\n",
    "    fixed_set_last_layer = model.predict(fixed_set,batch_size=10)\n",
    "    \n",
    "    return X_train_legit_last_layer,X_train_phish_last_layer,fixed_set_last_layer\n",
    "\n",
    "# Compute distance between the query set and all training examples \n",
    "def compute_all_distances(fixed_set,train_legit,train_phish):\n",
    "    train_size = train_legit.shape[0] + train_phish.shape[0]\n",
    "    X_all_train = np.concatenate((train_legit,train_phish))\n",
    "    pairwise_distance = np.zeros([fixed_set.shape[0],train_size])\n",
    "    for i in range(0,fixed_set.shape[0]):\n",
    "        pair1 = fixed_set[i,:]\n",
    "        for j in range(0,train_size):\n",
    "            pair2 = X_all_train[j,:]\n",
    "            l2_diff = compute_distance_pair(pair1,pair2)\n",
    "            pairwise_distance[i,j] = l2_diff\n",
    "    return pairwise_distance\n",
    "\n",
    "# Get index of false positives (different-website examples with small distance) of one query image\n",
    "def find_n_false_positives(distances,n,test_label):\n",
    "    count = 0\n",
    "    X_false_pos_idx = np.zeros([n,])\n",
    "    idx_min = np.argsort(distances)\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        next_min_idx = idx_min[i]\n",
    "        n_label = y_train[next_min_idx]\n",
    "        #false positives (have close distance even if they are from differenet category)\n",
    "        if not (test_label == n_label):\n",
    "            X_false_pos_idx[count] = next_min_idx\n",
    "            count = count + 1\n",
    "            if count == n:\n",
    "                break \n",
    "    while count < n:\n",
    "        idx_min[count] = -1\n",
    "        count = count + 1\n",
    "    return X_false_pos_idx\n",
    "\n",
    "# Get index of false negatives (same-website examples with large distance) of one query image\n",
    "def find_n_false_negatives(distances,n,test_label):\n",
    "    count = 0     \n",
    "    X_false_neg_idx = np.zeros([n,])\n",
    "    idx_max = np.argsort(distances)[::-1]\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        next_max_idx = idx_max[i]\n",
    "        n_label = y_train[next_max_idx]\n",
    "        #false negatives (have large distance although they are in the same category )\n",
    "        if test_label == n_label:\n",
    "            X_false_neg_idx[count] = next_max_idx\n",
    "            count = count + 1\n",
    "            if count == n:\n",
    "                break\n",
    "    while count < n:\n",
    "        idx_max[count] = -1\n",
    "        count = count + 1\n",
    "    return X_false_neg_idx\n",
    "\n",
    "# Get the idx of false positives and false negtaives for all query examples\n",
    "def find_index_for_all_set(distances,n):\n",
    "    all_idx = np.zeros([distances.shape[0],2,n])\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        distance_i = distances[i,:]\n",
    "        all_idx[i,0,:] = find_n_false_positives(distance_i,n,i)\n",
    "        all_idx[i,1,:] = find_n_false_negatives(distance_i,n,i)\n",
    "    return all_idx\n",
    "\n",
    "# Form the new training set based on the hard examples indices of all query images\n",
    "def find_next_training_set(all_idx,n):\n",
    "    global X_train_new, y_train_new\n",
    "    all_idx = all_idx.astype(int)\n",
    "    count = 0\n",
    "    for i in range(all_idx.shape[0]):\n",
    "        for j in range(0,n):\n",
    "            if not all_idx[i,0,j] == -1:\n",
    "                X_train_new[count,:,:,:] = X_train[all_idx[i,0,j],:,:,:]\n",
    "                y_train_new[count,:] = y_train[all_idx[i,0,j]]\n",
    "                count = count +1 \n",
    "        for j in range(0,n):\n",
    "            if not all_idx[i,1,j] == -1:\n",
    "                X_train_new[count,:,:,:] = X_train[all_idx[i,1,j],:,:,:]\n",
    "                y_train_new[count,:] = y_train[all_idx[i,1,j]]\n",
    "                count = count +1 \n",
    "    X_train_new = X_train_new[0:count,:]\n",
    "    y_train_new = y_train_new[0:count,:]\n",
    "    return X_train_new,y_train_new\n",
    "\n",
    "# Main function for subset sampling\n",
    "# Steps:\n",
    "    # Predict all images\n",
    "    # Find pairwise distances between query and training set\n",
    "    # Find indices of hard positive and negative examples\n",
    "    # Find new training set\n",
    "    # Order training set by targets\n",
    "def find_main_train(model,fixed_set,targets):\n",
    "    X_train_legit_last_layer,X_train_phish_last_layer,fixed_set_last_layer = predict_all_imgs(model)\n",
    "    pairwise_distance = compute_all_distances(fixed_set_last_layer,X_train_legit_last_layer,X_train_phish_last_layer)\n",
    "    n = 1\n",
    "    all_idx = find_index_for_all_set(pairwise_distance,n)\n",
    "    X_train_new,y_train_new = find_next_training_set(all_idx,n)\n",
    "    X_train_new,y_train_new = order_random_array(X_train_new,y_train_new,targets)\n",
    "    labels_start_end_train = start_end_each_target(targets,y_train_new)\n",
    "    return X_train_new,y_train_new,labels_start_end_train"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:26:51.846623Z",
     "start_time": "2025-01-20T23:26:51.836350Z"
    }
   },
   "source": [
    "# Don't sample negative image from the same category as the positive image (e.g. google and google drive)\n",
    "# Create clusters of same-company websites (e.g. all microsoft websites)\n",
    "\n",
    "\n",
    "targets_file = open(data_path/'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "        \n",
    "#targets names of parent and sub websites\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx \n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Sampling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:26:51.905932Z",
     "start_time": "2025-01-20T23:26:51.895907Z"
    }
   },
   "source": [
    "def pick_first_img_idx(labels_start_end,num_targets):\n",
    "    random_target = -1\n",
    "    while (random_target == -1):\n",
    "        random_target = np.random.randint(low = 0,high = num_targets)\n",
    "        if labels_start_end[random_target,0] == -1:\n",
    "            random_target = -1\n",
    "    return random_target\n",
    "\n",
    "def pick_pos_img_idx(img_label):\n",
    "    class_idx_start_end = labels_start_end_train[img_label,:]\n",
    "    same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_new[same_idx,:]\n",
    "    return img\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "    \n",
    "    class_idx_start_end = labels_start_end_train[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_new[idx_from_diff_target,:]\n",
    "    \n",
    "    return img,diff_target\n",
    "\n",
    "#Sample batch \n",
    "def get_batch(batch_size,train_fixed_set,num_targets):\n",
    "   \n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train,num_targets)\n",
    "        triple[0][i,:,:,:] = train_fixed_set[img_idx_pair1,:]\n",
    "        img_label = img_idx_pair1\n",
    "        \n",
    "        #get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(img_label)\n",
    "            \n",
    "        #get image for the thrid: negative from legit\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "          \n",
    "    return triple"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:27:07.668254Z",
     "start_time": "2025-01-20T23:26:51.969561Z"
    }
   },
   "source": [
    "\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "full_model = load_model(output_dir/f'{saved_model_name}.h5', custom_objects={'loss': loss})\n",
    "\n",
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "full_model.compile(loss=custom_loss(margin),optimizer=optimizer)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 00:26:52.094078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:52.121652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:52.121704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:52.122700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-21 00:26:52.124035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:52.124181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:52.124232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:53.531767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:53.531842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:53.531851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-01-21 00:26:53.531886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-21 00:26:53.531915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3906 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/jarcin/inz/src/models/visualphishnet/.venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T23:27:07.710336Z",
     "start_time": "2025-01-20T23:27:07.706425Z"
    }
   },
   "source": [
    "def save_keras_model(model):\n",
    "    model.save(output_dir/f'{new_saved_model_name}.h5')\n",
    "    print(\"Saved model to disk\")"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T00:04:38.922247Z",
     "start_time": "2025-01-20T23:28:09.729741Z"
    }
   },
   "source": [
    "n=1 #number of wrong points \n",
    "\n",
    "#all training images\n",
    "X_train = np.concatenate([X_train_legit,X_train_phish])\n",
    "y_train = np.concatenate([y_train_legit,y_train_phish])\n",
    "\n",
    "#subset training\n",
    "X_train_new = np.zeros([num_targets*2*n,X_train_legit.shape[1],X_train_legit.shape[2],X_train_legit.shape[3]])\n",
    "y_train_new = np.zeros([num_targets*2*n,1])\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "tot_count = 0 \n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"\\n ------------- \\n\")\n",
    "for k in range(0,num_sets):\n",
    "    print(\"Starting a new set!\")\n",
    "    print(\"\\n ------------- \\n\")\n",
    "    X_train_legit = all_imgs_train\n",
    "    y_train_legit = all_labels_train\n",
    "    \n",
    "    fixed_set_idx = find_fixed_set_idx(num_targets)\n",
    "    fixed_set = X_train_legit[fixed_set_idx.astype(int),:,:,:]\n",
    "    \n",
    "    for j in range(0,iter_per_set):\n",
    "        model = full_model.layers[3]\n",
    "        X_train_new,y_train_new,labels_start_end_train = find_main_train(model,fixed_set,num_targets)\n",
    "        for i in range(1, n_iter):\n",
    "            tot_count = tot_count + 1\n",
    "            inputs=get_batch(batch_size,fixed_set,num_targets)\n",
    "            loss_iteration=full_model.train_on_batch(inputs,targets_train)\n",
    "            \n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_iteration))\n",
    "\n",
    "            if tot_count % save_interval == 0:\n",
    "                save_keras_model(full_model)\n",
    "\n",
    "            if tot_count % lr_interval == 0:\n",
    "                start_lr = 0.99*start_lr\n",
    "                K.set_value(full_model.optimizer.lr, start_lr)\n",
    "                # wandb.config.update({\n",
    "                #     'training_params': {\n",
    "                #         'start_lr': start_lr  # New learning rate value\n",
    "                #     }\n",
    "                # }, allow_val_change=True)\n",
    "\n",
    "save_keras_model(full_model) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "\n",
      " ------------- \n",
      "\n",
      "Starting a new set!\n",
      "\n",
      " ------------- \n",
      "\n",
      "42/42 [==============================] - 2s 52ms/step\n",
      "7/7 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      " ------------- \n",
      "\n",
      "Iteration: 1. Loss: 9.086494445800781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m full_model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m     27\u001B[0m X_train_new,y_train_new,labels_start_end_train \u001B[38;5;241m=\u001B[39m find_main_train(model,fixed_set,num_targets)\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_iter):\n\u001B[1;32m     29\u001B[0m     tot_count \u001B[38;5;241m=\u001B[39m tot_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     30\u001B[0m     inputs\u001B[38;5;241m=\u001B[39mget_batch(batch_size,fixed_set,num_targets)\n",
      "Cell \u001B[0;32mIn[18], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m full_model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m     27\u001B[0m X_train_new,y_train_new,labels_start_end_train \u001B[38;5;241m=\u001B[39m find_main_train(model,fixed_set,num_targets)\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_iter):\n\u001B[1;32m     29\u001B[0m     tot_count \u001B[38;5;241m=\u001B[39m tot_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     30\u001B[0m     inputs\u001B[38;5;241m=\u001B[39mget_batch(batch_size,fixed_set,num_targets)\n",
      "File \u001B[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/inz/src/models/visualphishnet/.venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, exception_type)\u001B[0m\n\u001B[1;32m   2182\u001B[0m             from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_custom_thread_id)\n\u001B[1;32m   2184\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001B[0;32m-> 2185\u001B[0m         keep_suspended \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrace_suspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframes_tracker\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2187\u001B[0m frames_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m keep_suspended:\n\u001B[1;32m   2190\u001B[0m     \u001B[38;5;66;03m# This means that we should pause again after a set next statement.\u001B[39;00m\n",
      "File \u001B[0;32m~/inz/src/models/visualphishnet/.venv/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001B[0m\n\u001B[1;32m   2251\u001B[0m                 queue\u001B[38;5;241m.\u001B[39mput(internal_cmd)\n\u001B[1;32m   2252\u001B[0m                 wait_timeout \u001B[38;5;241m=\u001B[39m TIMEOUT_FAST\n\u001B[0;32m-> 2254\u001B[0m         \u001B[43mnotify_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwait_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2255\u001B[0m         notify_event\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m   2257\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.8.20-linux-x86_64-gnu/lib/python3.8/threading.py:558\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    556\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 558\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m~/.local/share/uv/python/cpython-3.8.20-linux-x86_64-gnu/lib/python3.8/threading.py:306\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 306\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    308\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m waiter\u001B[38;5;241m.\u001B[39macquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_model = full_model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir+'whitelist_emb2',whitelist_emb)\n",
    "np.save(output_dir+'whitelist_labels2',y_train_legit )\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir+'phishing_emb2',phishing_emb)\n",
    "np.save(output_dir+'phishing_labels2',all_labels_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
