{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 23:26:30.342786: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-16 23:26:31.075242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-16 23:26:31.075319: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-01-16 23:26:31.075326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2 as VP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters \n",
    "dataset_path = '../../../data/processed/smallerSampleDataset/'\n",
    "reshape_size = [224,224,3]\n",
    "phishing_test_size = 0.4\n",
    "# num_targets = 155 \n",
    "num_targets = 5\n",
    "\n",
    "# Model parameters\n",
    "input_shape = [224,224,3]\n",
    "margin = 2.2\n",
    "new_conv_params = [5,5,512]\n",
    "\n",
    "# Training parameters\n",
    "start_lr = 0.00002\n",
    "output_dir = '../../../notebooks/'\n",
    "saved_model_name = 'model' #from first training \n",
    "new_saved_model_name = 'model2'\n",
    "save_interval = 2000\n",
    "batch_size = 32 \n",
    "n_iter = 50000\n",
    "lr_interval = 250\n",
    "# hard examples training \n",
    "num_sets = 100\n",
    "iter_per_set = 8\n",
    "n_iter = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset:\n",
    "- Load training screenshots per website\n",
    "- Load Phishing screenshots per website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read_imgs_per_website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/processed/smallerSampleDataset/trusted_list/absa\n",
      "../../../data/processed/smallerSampleDataset/trusted_list/alibaba\n",
      "../../../data/processed/smallerSampleDataset/trusted_list/amazon\n",
      "../../../data/processed/smallerSampleDataset/trusted_list/apple\n",
      "../../../data/processed/smallerSampleDataset/trusted_list/boa\n",
      "../../../data/processed/smallerSampleDataset/phishing/absa\n",
      "../../../data/processed/smallerSampleDataset/phishing/alibaba\n",
      "../../../data/processed/smallerSampleDataset/phishing/amazon\n",
      "../../../data/processed/smallerSampleDataset/phishing/apple\n",
      "../../../data/processed/smallerSampleDataset/phishing/boa\n"
     ]
    }
   ],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'trusted_list/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "# imgs_num = 9363\n",
    "imgs_num = 420\n",
    "\n",
    "all_imgs_train,all_labels_train,all_file_names_train = VP.read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "# imgs_num = 1195\n",
    "imgs_num = 160\n",
    "\n",
    "all_imgs_test,all_labels_test,all_file_names_test = VP.read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Load the same train/split in phase 1\n",
    "phish_test_idx = np.load(output_dir+'test_idx.npy')\n",
    "phish_train_idx = np.load(output_dir+'train_idx.npy')\n",
    "\n",
    "X_test_phish = all_imgs_test[phish_test_idx,:]\n",
    "y_test_phish = all_labels_test[phish_test_idx,:]\n",
    "\n",
    "X_train_phish = all_imgs_test[phish_train_idx,:]\n",
    "y_train_phish = all_labels_test[phish_train_idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order and label targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_start_end_train_legit = VP.all_targets_start_end(num_targets,y_train_legit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Hard subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't sample negative image from the same category as the positive image (e.g. google and google drive)\n",
    "# Create clusters of same-company websites (e.g. all microsoft websites)\n",
    "\n",
    "\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "\n",
    "        \n",
    "#targets names of parent and sub websites\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "\n",
    "\n",
    "parents_ids,sub_target_lists_idx  = VP.get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 23:30:50.215650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.231693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.231734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.232253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-16 23:30:50.233917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.233977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.234001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.992095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.992161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.992168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-01-16 23:30:50.992196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 23:30:50.992224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3906 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/jarcin/inz/src/models/visualphishnet/.venv/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "full_model = load_model(output_dir+saved_model_name+'.h5', custom_objects={'loss': VP.loss})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "full_model.compile(loss=VP.custom_loss(margin),optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fixed_set_idx(labels_start_end_train_legit2, num_target):\n",
    "    website_random_idx = np.zeros([num_target,])\n",
    "    for i in range(0,num_target):\n",
    "        class_idx_start_end = labels_start_end_train_legit2[i,:]\n",
    "        website_random_idx[i] = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    return website_random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "\n",
      " ------------- \n",
      "\n",
      "Starting a new set!\n",
      "\n",
      " ------------- \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_legit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,iter_per_set):\n\u001B[1;32m     26\u001B[0m     model \u001B[38;5;241m=\u001B[39m full_model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m---> 27\u001B[0m     X_train_new,y_train_new,labels_start_end_train \u001B[38;5;241m=\u001B[39m \u001B[43mVP\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_main_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mfixed_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnum_targets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_iter):\n\u001B[1;32m     29\u001B[0m         tot_count \u001B[38;5;241m=\u001B[39m tot_count \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/inz/src/models/visualphishnet/phase2.py:223\u001B[0m, in \u001B[0;36mfind_main_train\u001B[0;34m(model, fixed_set, targets)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfind_main_train\u001B[39m(model,fixed_set,targets):\n\u001B[0;32m--> 223\u001B[0m     X_train_legit_last_layer,X_train_phish_last_layer,fixed_set_last_layer \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_all_imgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    224\u001B[0m     pairwise_distance \u001B[38;5;241m=\u001B[39m compute_all_distances(fixed_set_last_layer,X_train_legit_last_layer,X_train_phish_last_layer)\n\u001B[1;32m    225\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/inz/src/models/visualphishnet/phase2.py:129\u001B[0m, in \u001B[0;36mpredict_all_imgs\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpredict_all_imgs\u001B[39m(model):\n\u001B[0;32m--> 129\u001B[0m     X_train_legit_last_layer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(\u001B[43mX_train_legit\u001B[49m,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m    130\u001B[0m     X_train_phish_last_layer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_train_phish,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m    131\u001B[0m     fixed_set_last_layer \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(fixed_set,batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_train_legit' is not defined"
     ]
    }
   ],
   "source": [
    "n=1 #number of wrong points \n",
    "\n",
    "#all training images\n",
    "X_train = np.concatenate([X_train_legit,X_train_phish])\n",
    "y_train = np.concatenate([y_train_legit,y_train_phish])\n",
    "\n",
    "#subset training\n",
    "X_train_new = np.zeros([num_targets*2*n,X_train_legit.shape[1],X_train_legit.shape[2],X_train_legit.shape[3]])\n",
    "y_train_new = np.zeros([num_targets*2*n,1])\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "tot_count = 0 \n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"\\n ------------- \\n\")\n",
    "for k in range(0,num_sets):\n",
    "    print(\"Starting a new set!\")\n",
    "    print(\"\\n ------------- \\n\")\n",
    "    X_train_legit = all_imgs_train\n",
    "    y_train_legit = all_labels_train\n",
    "    \n",
    "    fixed_set_idx = find_fixed_set_idx(labels_start_end_train_legit, num_targets)\n",
    "    fixed_set = X_train_legit[fixed_set_idx.astype(int),:,:,:]\n",
    "    \n",
    "    for j in range(0,iter_per_set):\n",
    "        model = full_model.layers[3]\n",
    "        X_train_new,y_train_new,labels_start_end_train = VP.find_main_train(model,fixed_set,num_targets)\n",
    "        for i in range(1, n_iter):\n",
    "            tot_count = tot_count + 1\n",
    "            inputs=VP.get_batch_for_phase2(batch_size, fixed_set, num_targets)\n",
    "            loss_iteration=full_model.train_on_batch(inputs,targets_train)\n",
    "            \n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_iteration))\n",
    "\n",
    "            if tot_count % save_interval == 0:\n",
    "                VP.save_model(full_model)\n",
    "\n",
    "            if tot_count % lr_interval ==0:\n",
    "                start_lr = 0.99*start_lr\n",
    "                K.set_value(full_model.optimizer.lr, start_lr)\n",
    "\n",
    "VP.save_model(full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_model = full_model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir+'whitelist_emb2',whitelist_emb)\n",
    "np.save(output_dir+'whitelist_labels2',y_train_legit )\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir+'phishing_emb2',phishing_emb)\n",
    "np.save(output_dir+'phishing_labels2',all_labels_test )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
